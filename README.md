# NLP self-evaluation
# NLP 自我评价

**姓名**：黄俊尧  
**学号**：42311176  
**班级**：智能金融  
**自我评价**：98 分  

---

## 一、知识掌握情况

### 1. 系统性学习并深入理解 LSTM 与 Transformer 模型

在本课程学习过程中，我对序列建模领域的经典模型 **LSTM** 与 **Transformer** 进行了系统且深入的学习。学习路径上，我首先借助 B 站 up 主 **StatQuest** 的教学视频，对模型的直观动机与核心思想建立了清晰认知（相关合集见：https://www.bilibili.com/video/BV1zD421N7nA）。在此基础上，我进一步通过中英文博客精读以及原始论文阅读，对模型的数学细节进行了补充和推导。

- **LSTM**  
  原始论文：https://arxiv.org/pdf/1506.04214.pdf  
  博客参考：https://blog.csdn.net/qq_32172681/article/details/100061068  

- **Transformer**  
  原始论文：https://arxiv.org/abs/1706.03762  
  博客参考：https://zhuanlan.zhihu.com/p/607423406  

在学习过程中，我重点关注了模型提出的**动机**、**结构设计背后的数学原理**以及其**优势与局限性如何在公式推导中体现**。例如，LSTM 如何通过门控机制缓解梯度消失问题，而 Transformer 又是如何通过自注意力机制实现并行计算并提升长序列建模能力。相较于停留在“会用”的层面，我更注重对模型“为什么这样设计”的理解。

---

### 2. 对 GPT 系列模型演进的理解与实践

在大模型相关内容上，我系统梳理了 **GPT-1 到 GPT-5** 的技术演进脉络，包括模型规模、训练范式、对齐方式及能力边界的变化。同时，我基于 **莎士比亚全集**，从零训练并调试了一个 **nanoGPT**，在实践中加深了对 Transformer 架构、语言模型训练流程以及超参数影响的理解。

---

### 3. 变分自编码器（VAE）的理论掌握

我较为系统地学习了 **变分自编码器（VAE）** 的基本原理，理解其通过编码器—解码器结构对数据分布进行概率建模的思想。在理论层面，我能够推导并解释 **ELBO 目标函数** 的来源，清楚 **KL 散度** 在正则化潜变量分布、约束后验近似中的作用，同时也理解了 **重参数化技巧** 在保证反向传播可导性方面的关键意义。

在学习 VAE 的过程中，我同步补充了对 KL 散度及相关信息论概念的理解，并参考了个人认为讲解十分清晰的视频资源：https://www.bilibili.com/video/BV1xFxMz1EMS。这一部分学习显著提升了我对概率生成模型的整体认知。

---

### 4. 课内其他知识与作业完成情况

对于课程中涉及的其他 NLP 基础知识与相关作业，我均进行了系统补齐与巩固，确保对课程整体内容形成较为完整、连贯的理解框架。

---

## 二、课外探索与拓展学习

### 1. 多模态学习及多模态不平衡问题的深入探索

在课外学习中，我重点关注了 **多模态学习（Multimodal Learning）**，并对其中的 **多模态不平衡问题** 产生了浓厚兴趣。基于论文 *BalanceBenchmark: A Survey for Multimodal Imbalance Learning*，我系统梳理了多模态模型在应对模态不平衡时的四大优化方向：

- **数据层面**：通过数据重采样、增强或预处理缓解模态分布不均；
- **建模层面**：在特征提取或融合阶段引入动态调节机制，抑制强模态、增强弱模态的表达能力，其中对编码器结构的优化尤为常见；
- **目标函数层面**：通过设计新的损失函数或引入模态约束，使不同模态在优化目标中获得更均衡的梯度信号；
- **优化策略层面**：在反向传播阶段对梯度或参数更新进行调制，减弱强模态主导效应，从而提升弱模态的学习效果。

我的阅读重点主要集中在 **目标函数优化方向** 的相关论文，同时也涉及部分编码器结构改进的方法。针对多篇被反复引用的经典论文以及个人兴趣较强的前沿工作，我撰写了较为详细的论文解读，内容包含必要的数学推导与方法分析（详见：https://github.com/JunyaoHwang/NLP-self_evaluation）。

此外，我还复现了论文 **《I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts》**，并在实验过程中进一步验证了混合专家模型之间是否仍然存在 **梯度冲突问题**。在这一过程中，我补充学习了大量前置知识，包括 **PID 信息论、费舍尔信息矩阵、多模态编码层的多种实现方式** 等。持续的论文解读与复现训练，在无形中帮助我巩固了理论基础，也显著提升了技术写作与逻辑表达能力。

---

### 2. 大语言模型（LLM）的反思与自我改进机制研究

除多模态方向外，我还阅读了大量关于 **LLM 开环与闭环反思机制** 的相关论文，包括 **Self-Refine**（https://arxiv.org/abs/2303.17651）、**Reflexion**（https://arxiv.org/abs/2303.11366）等工作，重点关注模型如何通过自我反馈或外部反馈不断优化生成质量。

其中，我重点研读了 **DORA 方法**（https://aclanthology.org/2025.coling-main.504/），该方法通过引入一个小规模的 prompter 模型，并结合外部反馈对 prompter 进行优化，从而实现 prompt 的动态调整。这一思路让我对“模型能力提升不完全依赖参数规模增长”有了更深入的理解。此外，我也系统学习了 **Gemini 提示词指南**（https://mp.weixin.qq.com/s/WOhzAshpLmRCuNhFleHPNg），进一步加深了我对 Prompt Engineering 的结构化认知。

### 3. 细粒度识别任务尝试

在本学期初期，我基于 TransFG（Fine-grained Visual Recognition with Transformers，https://arxiv.org/abs/2103.07976）这一细粒度识别模型，并结合 SED（https://arxiv.org/abs/2407.02778）提出的噪声样本处理方法，独立设计并实现了一个 TransFG + SED 的细粒度识别整体框架。

该框架旨在在保持 Transformer 对局部判别区域建模能力的同时，引入 SED 对噪声样本的建模与抑制机制，从而缓解细粒度识别任务中由标注噪声和样本质量不均带来的性能退化问题。尽管该方法在整体结构与训练策略上仍有进一步优化空间，但在实验结果上，相较于原始 baseline 模型已取得了显著性能提升，验证了将噪声建模机制引入细粒度识别任务中的有效性。代码详情在(https://github.com/JunyaoHwang/NLP-self_evaluation/tree/main/%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86SED )
